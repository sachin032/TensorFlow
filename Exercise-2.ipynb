{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1hGUpEBSivzE"
   },
   "source": [
    "# **Download Required third party dependancies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T11:51:41.921787Z",
     "start_time": "2020-03-10T11:51:35.970662Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  !pip install -q fuzzywuzzy metaphone whoosh jellyfish\n",
    "except Exception:\n",
    "    pass\n",
    "#\n",
    "# import tensorflow as tf\n",
    "# !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "# !unzip ngrok-stable-linux-amd64.zip\n",
    "# !pip install tensorboardcolab\n",
    "# !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "# !unzip ngrok-stable-linux-amd64.zip\n",
    "# LOG_DIR = './log'\n",
    "# get_ipython().system_raw('tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'.format(LOG_DIR))\n",
    "# get_ipython().system_raw('./ngrok http 6006 &')\n",
    "# ! curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n",
    "# # **Load the Data set**\n",
    "# from google.colab import files\n",
    "# data =  files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V31nPf4uiI5h"
   },
   "source": [
    "# **Load Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T11:51:44.554184Z",
     "start_time": "2020-03-10T11:51:44.517835Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ui_V6U-x_VG5",
    "outputId": "679a7557-4ef9-491c-abda-a009a3b11d5e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/anaconda3/lib/python3.7/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pathlib\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, hashing_trick, one_hot, text_to_word_sequence\n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from jellyfish import jaro_winkler, levenshtein_distance, soundex\n",
    "from whoosh.analysis import StandardAnalyzer\n",
    "from metaphone import doublemetaphone\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9owgBBGNiRD1"
   },
   "source": [
    "# **Taxonomy Declaration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T11:51:47.716014Z",
     "start_time": "2020-03-10T11:51:47.703965Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "RVMqM1EqglR3"
   },
   "outputs": [],
   "source": [
    "ACCOUNT_NAME = 'accountName'\n",
    "ACCOUNT_KEY = 'accountKey'\n",
    "CONTAINER_NAME = 'containerName'\n",
    "ANALYTICS_EXTERNAL = 'analytics-external'\n",
    "BLOB_END_SUFFIX = '.blob.core.windows.net'\n",
    "BLOB_PREFIX = 'fs.azure.account.key.'\n",
    "ANALYTICS_INTERNAL = 'analytics-internal'\n",
    "LEVENSHTEIN = 'levenshtein'\n",
    "SOUNDEX = 'soundex'\n",
    "OVERLAP = 'overlap'\n",
    "OVERLAPLEVENSHTEIN = 'overlapLevenshtein'\n",
    "JACCARD = 'jaccard'\n",
    "JACCARDLEVENSHTEIN = 'jaccardLevenshtein'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T11:51:50.688339Z",
     "start_time": "2020-03-10T11:51:49.832028Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "xg_tVH_jTgdb"
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4)\n",
    "\n",
    "df = pd.read_csv(\"/home/sachin/#Riversand/Match_Non_Match/BigData.csv\")\n",
    "\n",
    "df['system_universalbusinessnumber_11'] = df[\n",
    "    'system_universalbusinessnumber_11'].astype(str)\n",
    "\n",
    "df['system_universalbusinessnumber_12'] = df[\n",
    "    'system_universalbusinessnumber_11'].astype(str)\n",
    "\n",
    "df = df.drop([\n",
    "    'feature_14', 'feature_15', 'feature_16', 'feature_17', 'feature_18',\n",
    "    'feature_19', 'feature_20', 'feature_21'\n",
    "],\n",
    "             axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T11:52:33.648459Z",
     "start_time": "2020-03-10T11:52:33.636133Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "SuKPe0Q4XbSt"
   },
   "outputs": [],
   "source": [
    "algos = {\n",
    "    'jaro_winkler': [],\n",
    "    'exact_match': [],\n",
    "    'overlapLevenshtein': [\n",
    "        'system_businessname', 'system_address', 'system_alternatename',\n",
    "        'system_phonenumber', 'system_universalbusinessnumber'\n",
    "    ],\n",
    "    'soundex': ['system_businessname', 'system_alternatename']\n",
    "}\n",
    "\n",
    "cols = [\n",
    "    'id', 'system_businessname_1', 'system_businessname_2',\n",
    "    'system_alternatename_3', 'system_alternatename_4', 'system_address_5',\n",
    "    'system_address_6', 'system_phonenumber_7', 'system_phonenumber_8',\n",
    "    'system_dateofinception_9', 'system_dateofinception_10',\n",
    "    'system_universalbusinessnumber_11', 'system_universalbusinessnumber_12',\n",
    "    'LABEL'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YiJ8kv8SinXC"
   },
   "source": [
    "# **Data Preprocessing Steps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T11:52:38.023704Z",
     "start_time": "2020-03-10T11:52:37.953209Z"
    },
    "code_folding": [
     0,
     5,
     20,
     29,
     57,
     74,
     86,
     93,
     102,
     129,
     161,
     176,
     181,
     196,
     210,
     224,
     238,
     252
    ],
    "colab": {},
    "colab_type": "code",
    "id": "QtX1qFZ3Wkhr"
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    analyzer = StandardAnalyzer()\n",
    "    return [t.text for t in analyzer(text)]\n",
    "\n",
    "\n",
    "def overlap(A, B):\n",
    "    try:\n",
    "        if A is None or B is None or len(A) == 0 or len(B) == 0:\n",
    "            return 0.0\n",
    "        setA = tokenize(A)\n",
    "        setB = tokenize(B)\n",
    "        num_intersection = setA.intersection(setB)\n",
    "        min_len = len(setA)\n",
    "        if (min_len > len(setB)):\n",
    "            min_len = len(setB)\n",
    "        return float(intersection) / min_len\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def jaccard(A, B):\n",
    "    if A is None or B is None or len(A) == 0 or len(B) == 0:\n",
    "        return 0.0\n",
    "    setA = tokenize(A)\n",
    "    setB = tokenize(B)\n",
    "    num_intersection = setA.intersection(setB)\n",
    "    return float(intersection) / (len(setA) + len(setB) - intersection)\n",
    "\n",
    "\n",
    "def overlap_levenshtein(A, B):\n",
    "    try:\n",
    "        if A is None or B is None or len(A) == 0 or len(B) == 0:\n",
    "            return 0.0\n",
    "        setA = set(tokenize(A))\n",
    "        setB = set(tokenize(B))\n",
    "        num_intersection = len(setA.intersection(setB))\n",
    "        max_dist = 0\n",
    "        for wordA in setA:\n",
    "            max_dist = 0\n",
    "            for wordB in setB:\n",
    "                if wordA not in list(\n",
    "                        setA.intersection(setB)) and wordB not in list(\n",
    "                            setA.intersection(setB)):\n",
    "                    dist = levenshtein_distance_metric(wordA, wordB)\n",
    "                    if dist > max_dist and dist > 0.75:\n",
    "                        max_dist = dist\n",
    "            num_intersection = num_intersection + max_dist\n",
    "        min_len = len(setA)\n",
    "        if (min_len > len(setB)):\n",
    "            min_len = len(setB)\n",
    "        if min_len == 0:\n",
    "            return 0.0\n",
    "        return float(num_intersection) / min_len\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def jaccard_levenshtein(A, B):\n",
    "    if A is None or B is None or len(A) == 0 or len(B) == 0:\n",
    "        return 0.0\n",
    "    setA = tokenize(A)\n",
    "    setB = tokenize(B)\n",
    "    num_intersection = len(setA.intersection(setB))\n",
    "    for wordA in setA:\n",
    "        max_dist = 0\n",
    "        for wordB in setB:\n",
    "            if wordB not in setA.intersection(setB):\n",
    "                dist = levenshtein_distance_metric(wordA, wordB)\n",
    "                if dist > max_dist and max_dist > 0.75:\n",
    "                    max_dist = dist\n",
    "            num_intersection = num_intersection + max_dist\n",
    "    return float(num_intersection) / (len(setA) + len(setB) - num_intersection)\n",
    "\n",
    "\n",
    "def levenshtein_distance_metric(A, B):\n",
    "    try:\n",
    "        if A is None or B is None or len(A) == 0 or len(B) == 0:\n",
    "            return 0.0\n",
    "        max_len = len(A)\n",
    "        if max_len < len(B):\n",
    "            max_len = len(B)\n",
    "        return 1 - float(levenshtein_distance(A, B)) / max_len\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def jaro_winkler_metric(A, B):\n",
    "    if A is None or B is None or len(A) == 0 or len(B) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return float(jaro_winkler(A.lower(), B.lower()))\n",
    "\n",
    "\n",
    "def exact_metric(A, B):\n",
    "    if A is None or B is None:\n",
    "        return 0.0\n",
    "    if A == B:\n",
    "        return 0.25\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def overlap_levenshtein(A, B):\n",
    "    try:\n",
    "        if A is None or B is None or len(A) == 0 or len(B) == 0:\n",
    "            return 0.0\n",
    "        setA = set(tokenize(A))\n",
    "        setB = set(tokenize(B))\n",
    "        num_intersection = len(setA.intersection(setB))\n",
    "        intersected_word = list(setA.intersection(setB))\n",
    "        max_dist = 0\n",
    "        for wordA in setA:\n",
    "            max_dist = 0\n",
    "            for wordB in setB:\n",
    "                if wordA not in intersected_word and wordB not in intersected_word:\n",
    "                    dist = levenshtein_distance_metric(wordA, wordB)\n",
    "                    if dist > max_dist and dist > 0.75:\n",
    "                        max_dist = dist\n",
    "            num_intersection = num_intersection + max_dist\n",
    "        min_len = len(setA)\n",
    "        if (min_len > len(setB)):\n",
    "            min_len = len(setB)\n",
    "        if min_len == 0:\n",
    "            return 0.0\n",
    "        return float(num_intersection) / min_len\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "def soundex_metric(A, B):\n",
    "    try:\n",
    "        if A is None or B is None or A is \"\" or B is \"\" or len(A) == 0 or len(\n",
    "                B) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        setA = tokenize(A)\n",
    "        setB = tokenize(B)\n",
    "        soundexA = set()\n",
    "        soundexB = set()\n",
    "        for wordA in setA:\n",
    "            soundexA.add(soundex(wordA))\n",
    "\n",
    "        for wordB in setB:\n",
    "            soundexB.add(soundex(wordB))\n",
    "\n",
    "        intersection = 0\n",
    "        for wordA in soundexA:\n",
    "            if wordA in soundexB:\n",
    "                intersection = intersection + 1\n",
    "\n",
    "        min_len = len(soundexA)\n",
    "        if min_len > len(soundexB):\n",
    "            min_len = len(soundexB)\n",
    "        if min_len == 0.0:\n",
    "            return 0.0\n",
    "\n",
    "        return float(intersection) / (min_len)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def swapped_attribute(fir, sec, pair_of_header, func, header_index):\n",
    "    combinations = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "    metric_values = list()\n",
    "    for each_combination in combinations:\n",
    "        metric_values.append(\n",
    "            func(fir[header_index[pair_of_header[each_combination[0]]]],\n",
    "                 sec[header_index[pair_of_header[each_combination[1]]]]))\n",
    "    swp_1 = metric_values[0] + metric_values[3]\n",
    "    swp_2 = metric_values[1] + metric_values[2]\n",
    "    if swp_1 > swp_2:\n",
    "        return metric_values[0], metric_values[3]\n",
    "    else:\n",
    "        return metric_values[1], metric_values[2]\n",
    "\n",
    "\n",
    "def applyAlgorithms(dataframe, algorithms, column_list):\n",
    "    counter = 0\n",
    "    column_length = len(column_list)\n",
    "    for key, values_list in algorithms.items():\n",
    "        values_list.sort()\n",
    "        if key == LEVENSHTEIN:\n",
    "            for value in values_list:\n",
    "                column_name_list = [\n",
    "                    column_name for column_name in column_list\n",
    "                    if value in column_name\n",
    "                ]\n",
    "                col_name = value + \"_\" + str(column_length + counter)\n",
    "                df[col_name] = df.apply(\n",
    "                    lambda row: levenshtein_distance_metric(\n",
    "                        row['%s' % column_name_list[0]], row[\n",
    "                            '%s' % column_name_list[1]]),\n",
    "                    axis=1)\n",
    "                # dataframe = dataframe.withColumn(\"feature_\"+str(column_length+counter), apply_levenshtein(column_name_list[0], column_name_list[1]))\n",
    "                counter = counter + 1\n",
    "\n",
    "        elif key == SOUNDEX:\n",
    "            for value in values_list:\n",
    "                column_name_list = [\n",
    "                    column_name for column_name in column_list\n",
    "                    if value in column_name\n",
    "                ]\n",
    "                col_name = value + \"_\" + str(column_length + counter)\n",
    "                df[col_name] = df.apply(\n",
    "                    lambda row: soundex_metric(row['%s' % column_name_list[\n",
    "                        0]], row['%s' % column_name_list[1]]),\n",
    "                    axis=1)\n",
    "                # dataframe = dataframe.withColumn(\"feature_\"+str(column_length+counter), apply_soundex(column_name_list[0], column_name_list[1]))\n",
    "                counter = counter + 1\n",
    "\n",
    "        elif key == OVERLAP:\n",
    "            for value in values_list:\n",
    "                column_name_list = [\n",
    "                    column_name for column_name in column_list\n",
    "                    if value in column_name\n",
    "                ]\n",
    "                col_name = value + \"_\" + str(column_length + counter)\n",
    "                df[col_name] = df.apply(\n",
    "                    lambda row: overlap(row['%s' % column_name_list[0]], row[\n",
    "                        '%s' % column_name_list[1]]),\n",
    "                    axis=1)\n",
    "                # dataframe = dataframe.withColumn(\"feature_\"+str(column_length+counter), apply_overlap(column_name_list[0], column_name_list[1]))\n",
    "                counter = counter + 1\n",
    "\n",
    "        elif key == OVERLAPLEVENSHTEIN:\n",
    "            for value in values_list:\n",
    "                column_name_list = [\n",
    "                    column_name for column_name in column_list\n",
    "                    if value in column_name\n",
    "                ]\n",
    "                col_name = value + \"_\" + str(column_length + counter)\n",
    "                df[col_name] = df.apply(lambda row: overlap_levenshtein(\n",
    "                    row['%s' % column_name_list[0]], row['%s' %\n",
    "                                                         column_name_list[1]]),\n",
    "                                        axis=1)\n",
    "                # dataframe = dataframe.withColumn(\"feature_\"+str(column_length+counter), apply_overlap_levenshtein(column_name_list[0], column_name_list[1]))\n",
    "                counter = counter + 1\n",
    "\n",
    "        elif key == JACCARD:\n",
    "            for value in values_list:\n",
    "                column_name_list = [\n",
    "                    column_name for column_name in column_list\n",
    "                    if value in column_name\n",
    "                ]\n",
    "                col_name = value + \"_\" + str(column_length + counter)\n",
    "                df[col_name] = df.apply(\n",
    "                    lambda row: jaccard(row['%s' % column_name_list[0]], row[\n",
    "                        '%s' % column_name_list[1]]),\n",
    "                    axis=1)\n",
    "                # dataframe = dataframe.withColumn(\"feature_\"+str(column_length+counter), apply_jaccard(column_name_list[0], column_name_list[1]))\n",
    "                counter = counter + 1\n",
    "\n",
    "        if key == JACCARDLEVENSHTEIN:\n",
    "            for value in values_list:\n",
    "                column_name_list = [\n",
    "                    column_name for column_name in column_list\n",
    "                    if value in column_name\n",
    "                ]\n",
    "                col_name = value + \"_\" + str(column_length + counter)\n",
    "                df[col_name] = df.apply(lambda row: jaccard_levenshtein(\n",
    "                    row['%s' % column_name_list[0]], row['%s' %\n",
    "                                                         column_name_list[1]]),\n",
    "                                        axis=1)\n",
    "                # dataframe = dataframe.withColumn(\"feature_\"+str(column_length+counter), apply_jaccard_levenshtein(column_name_list[0], column_name_list[1]))\n",
    "                counter = counter + 1\n",
    "\n",
    "    return dataframe, counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OrUbJOexWXI2"
   },
   "source": [
    "# **Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T11:54:10.906153Z",
     "start_time": "2020-03-10T11:52:51.155163Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "phUPAWpBZ4vz"
   },
   "outputs": [],
   "source": [
    "bb = applyAlgorithms(dataframe=df, algorithms=algos, column_list=cols)\n",
    "\n",
    "df = bb[0]\n",
    "\n",
    "train_df = df[[\n",
    "    'system_address_14', 'system_alternatename_15', 'system_businessname_16',\n",
    "    'system_phonenumber_17', 'system_universalbusinessnumber_18',\n",
    "    'system_alternatename_19', 'system_businessname_20', 'LABEL'\n",
    "]]\n",
    "\n",
    "target = train_df.pop('LABEL')\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_df.values, target.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q6eM69BMi6_l"
   },
   "source": [
    "# **Model Building**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T11:55:00.919421Z",
     "start_time": "2020-03-10T11:55:00.854054Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "zQozkWS90jri"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T11:55:05.471833Z",
     "start_time": "2020-03-10T11:55:05.458436Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "2eYOPDpB0dKi",
    "outputId": "91910576-3f5d-42ee-9c2f-2bab72cc1804"
   },
   "outputs": [],
   "source": [
    "train_dataset = dataset.shuffle(len(df)).batch(1)\n",
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T12:04:04.786565Z",
     "start_time": "2020-03-10T11:59:14.604370Z"
    },
    "code_folding": [
     13
    ],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "colab_type": "code",
    "id": "Y6F0fTVy04nl",
    "outputId": "cf8365f7-121f-4657-a4fc-bd64f13317ed",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 155901 steps\n",
      "155901/155901 [==============================] - 290s 2ms/step - loss: 0.0000e+00 - accuracy: 0.7308\n"
     ]
    }
   ],
   "source": [
    "class NeuralNet():\n",
    "    \n",
    "    def get_trained_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(10, activation='relu', input_shape=(7, )))\n",
    "        model.add(Dense(10, activation='relu'))\n",
    "        model.add(Dense(10, activation='relu'))\n",
    "        model.add(Dense(10, activation='relu'))\n",
    "        model.add(Dense(10, activation='relu'))\n",
    "        model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "        logdir = \"/home/sachin/Desktop/logs\" + datetime.datetime.now(\n",
    "        ).strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = TensorBoard(log_dir=logdir)\n",
    "\n",
    "        model.compile(optimizer=RMSprop(learning_rate=0.01),\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "        logdir = \"/home/sachin/Desktop/logs\" + datetime.datetime.now(\n",
    "        ).strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = TensorBoard(log_dir=logdir)\n",
    "\n",
    "        history = model.fit(train_dataset,\n",
    "                            epochs=1,\n",
    "                            verbose=1,\n",
    "                            callbacks=[tensorboard_callback])\n",
    "\n",
    "        return history, model\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "net = NeuralNet()\n",
    "history, model = net.get_trained_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard import notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T09:18:31.770638Z",
     "start_time": "2020-02-06T09:18:31.724297Z"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_STORAGE = \"/home/sachin/Desktop/Model/\"\n",
    "model.save(MODEL_STORAGE+\"model_2.h5\")\n",
    "model.save_weights(MODEL_STORAGE+\"model_2_weights.h5\")\n",
    "model_json= model.to_json()\n",
    "with open(MODEL_STORAGE+\"model_2.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f66NhxBskjSy"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "loss = history.history['acc']\n",
    "val_loss = history.history['val_acc']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "plt.figure(figsize=(17, 10))\n",
    "plt.plot(epochs, loss, 'r', label='Training accuracy')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "21Ru5CEBjBVi"
   },
   "source": [
    "# **TensorFlow Dataset API for GPU and TPU processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ksFTnFuOgxw0"
   },
   "source": [
    "<br> \n",
    "**Load Data frame and Make Tensor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f8sGa54nmXn0"
   },
   "outputs": [],
   "source": [
    "data_slices = tf.data.Dataset.from_tensor_slices(dict(df))\n",
    "for feature_batch in data_slices.take(1):\n",
    "    for key, value in feature_batch.items():\n",
    "        print(\"  {!r:20s}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0yBAZjIwgHiQ"
   },
   "source": [
    "**Load CSV data and create a tensor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GFAyS6LtyS_6"
   },
   "outputs": [],
   "source": [
    "def get_dataset(file_path, **kwargs):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        file_path,\n",
    "        batch_size=5,  #Artificially small to make examples easier to show.\n",
    "        label_name=\"LABEL\",\n",
    "        na_value=\"?\",\n",
    "        num_epochs=1,\n",
    "        ignore_errors=True,\n",
    "        **kwargs)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "raw_train_data = get_dataset(\"/content/BigData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SMC-U-vHdlLZ"
   },
   "outputs": [],
   "source": [
    "def show_batch(dataset):\n",
    "    for batch, label in dataset.take(1):\n",
    "        for key, value in batch.items():\n",
    "            print(\"{:20s}: {}\".format(key, value.numpy()))\n",
    "\n",
    "\n",
    "show_batch(raw_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hgxUtIKshrb8"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iROIM3VBZRiC"
   },
   "outputs": [],
   "source": [
    "processed_df, counter = applyAlgorithms(dataframe=df,algorithms=algos,column_list=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T09:19:54.984538Z",
     "start_time": "2020-02-06T09:19:54.948404Z"
    }
   },
   "outputs": [],
   "source": [
    "class PlotLearning(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.acc = []\n",
    "        self.val_acc = []\n",
    "        self.fig = plt.figure()\n",
    "        \n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.acc.append(logs.get('acc'))\n",
    "        self.val_acc.append(logs.get('val_acc'))\n",
    "        self.i += 1\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2, sharex=True)\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        ax1.set_yscale('log')\n",
    "        ax1.plot(self.x, self.losses, label=\"loss\")\n",
    "        ax1.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "        ax1.legend()\n",
    "        \n",
    "        ax2.plot(self.x, self.acc, label=\"accuracy\")\n",
    "        ax2.plot(self.x, self.val_acc, label=\"validation accuracy\")\n",
    "        ax2.legend()\n",
    "        \n",
    "        plt.show();\n",
    "        \n",
    "plot = PlotLearning()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "60hBMzpOiCKW",
    "V31nPf4uiI5h",
    "9owgBBGNiRD1",
    "YiJ8kv8SinXC",
    "OrUbJOexWXI2",
    "21Ru5CEBjBVi"
   ],
   "name": "Tensorflow-NLP",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
