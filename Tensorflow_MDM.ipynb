{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow-MDM",
      "provenance": [],
      "collapsed_sections": [
        "60hBMzpOiCKW",
        "V31nPf4uiI5h",
        "9owgBBGNiRD1",
        "YiJ8kv8SinXC",
        "OrUbJOexWXI2",
        "21Ru5CEBjBVi"
      ],
      "authorship_tag": "ABX9TyM6Rx7mnWxXXwBcFriBlBWo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sachin032/Tensorflow/blob/master/Tensorflow_MDM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hGUpEBSivzE",
        "colab_type": "text"
      },
      "source": [
        "# **Download Required third party dependancies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_5md7Pdg8_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  !pip install -q tf-nightly fuzzywuzzy metaphone whoosh jellyfish\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvn-OT4wsu5u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "b3d8163f-7b2c-4804-8075-59138740500c"
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-05 09:56:07--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.204.140.35, 34.238.36.128, 52.7.241.210, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.204.140.35|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13773305 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.1’\n",
            "\n",
            "\r          ngrok-sta   0%[                    ]       0  --.-KB/s               \rngrok-stable-linux- 100%[===================>]  13.13M  85.1MB/s    in 0.2s    \n",
            "\n",
            "2020-02-05 09:56:07 (85.1 MB/s) - ‘ngrok-stable-linux-amd64.zip.1’ saved [13773305/13773305]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5oqVv8Esytl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "f5c76152-e2b0-492f-b979-e2d302fc1157"
      },
      "source": [
        "!pip install tensorboardcolab\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "\n",
        "\n",
        "\n",
        "LOG_DIR = './log'\n",
        "get_ipython().system_raw('tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'.format(LOG_DIR))\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardcolab in /usr/local/lib/python3.6/dist-packages (0.0.22)\n",
            "--2020-02-05 09:56:59--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.204.140.35, 34.238.36.128, 52.7.241.210, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.204.140.35|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13773305 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.2’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.13M  77.3MB/s    in 0.2s    \n",
            "\n",
            "2020-02-05 09:56:59 (77.3 MB/s) - ‘ngrok-stable-linux-amd64.zip.2’ saved [13773305/13773305]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: ngrok                   \n",
            "https://63d67953.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60hBMzpOiCKW",
        "colab_type": "text"
      },
      "source": [
        "# **Load the Data set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gW8M8vBRdoB",
        "colab_type": "code",
        "outputId": "9b8860e5-e142-453d-9f38-0ce8322dbdd6",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from google.colab import files\n",
        "data =  files.upload()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cdd49cba-56bd-4cb7-9023-fecc92df2b72\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-cdd49cba-56bd-4cb7-9023-fecc92df2b72\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving BigData.csv to BigData (3).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V31nPf4uiI5h",
        "colab_type": "text"
      },
      "source": [
        "# **Load Required Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ui_V6U-x_VG5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "679a7557-4ef9-491c-abda-a009a3b11d5e"
      },
      "source": [
        "import io\n",
        "import tensorflow as tf\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pathlib\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer,hashing_trick,one_hot,text_to_word_sequence\n",
        "\n",
        "from jellyfish import jaro_winkler, levenshtein_distance, soundex\n",
        "from whoosh.analysis import StandardAnalyzer\n",
        "from metaphone import doublemetaphone\n",
        "from fuzzywuzzy import fuzz\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
            "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9owgBBGNiRD1",
        "colab_type": "text"
      },
      "source": [
        "# **Taxonomy Declaration**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVMqM1EqglR3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ACCOUNT_NAME = 'accountName'\n",
        "ACCOUNT_KEY = 'accountKey'\n",
        "CONTAINER_NAME = 'containerName'\n",
        "ANALYTICS_EXTERNAL = 'analytics-external'\n",
        "BLOB_END_SUFFIX = \".blob.core.windows.net\"\n",
        "BLOB_PREFIX = \"fs.azure.account.key.\"\n",
        "ANALYTICS_INTERNAL = 'analytics-internal'\n",
        "LEVENSHTEIN = 'levenshtein'\n",
        "SOUNDEX = 'soundex'\n",
        "OVERLAP = 'overlap'\n",
        "OVERLAPLEVENSHTEIN = 'overlapLevenshtein'\n",
        "JACCARD = 'jaccard'\n",
        "JACCARDLEVENSHTEIN = 'jaccardLevenshtein'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xg_tVH_jTgdb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.set_printoptions(precision=4)\n",
        "df = pd.read_csv(io.StringIO(data['BigData.csv'].decode('utf-8')))\n",
        "df['system_universalbusinessnumber_11'] = df['system_universalbusinessnumber_11'].astype(str)\n",
        "df['system_universalbusinessnumber_12'] = df['system_universalbusinessnumber_11'].astype(str)\n",
        "df = df.drop([ 'feature_14','feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19','feature_20', 'feature_21'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuKPe0Q4XbSt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "algos = {\n",
        "          'jaro_winkler': [], \n",
        "          'exact_match': [], \n",
        "          'overlapLevenshtein': ['system_businessname', 'system_address', 'system_alternatename', 'system_phonenumber', 'system_universalbusinessnumber'], \n",
        "          'soundex': ['system_businessname', 'system_alternatename']\n",
        "        }\n",
        "\n",
        "cols = ['id', 'system_businessname_1', 'system_businessname_2',\n",
        "       'system_alternatename_3', 'system_alternatename_4', 'system_address_5',\n",
        "       'system_address_6', 'system_phonenumber_7', 'system_phonenumber_8',\n",
        "       'system_dateofinception_9', 'system_dateofinception_10',\n",
        "       'system_universalbusinessnumber_11',\n",
        "       'system_universalbusinessnumber_12', 'LABEL']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiJ8kv8SinXC",
        "colab_type": "text"
      },
      "source": [
        "# **Data Preprocessing Steps**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtX1qFZ3Wkhr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(text):\n",
        "  analyzer = StandardAnalyzer()\n",
        "  return [t.text for t in analyzer(text)]\n",
        "\n",
        "def overlap(A, B):\n",
        "  try:\n",
        "    if A is None or B is None or len(A) == 0 or len(B) == 0:\n",
        "      return 0.0\n",
        "    setA = tokenize(A)\n",
        "    setB = tokenize(B)\n",
        "    num_intersection = setA.intersection(setB)\n",
        "    min_len = len(setA)\n",
        "    if(min_len > len(setB)):\n",
        "      min_len = len(setB)\n",
        "    return float(intersection)/min_len\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "def jaccard(A, B):\n",
        "  if A is None or B is None or len(A) == 0 or len(B) == 0:\n",
        "    return 0.0\n",
        "  setA = tokenize(A)\n",
        "  setB = tokenize(B)\n",
        "  num_intersection = setA.intersection(setB)\n",
        "  return float(intersection)/(len(setA) + len(setB) - intersection)\n",
        "\n",
        "def overlap_levenshtein(A, B):\n",
        "  try:\n",
        "    if A is None or B is None or len(A) == 0 or len(B) == 0:\n",
        "      return 0.0\n",
        "    setA = set(tokenize(A))\n",
        "    setB = set(tokenize(B))\n",
        "    num_intersection = len(setA.intersection(setB))\n",
        "    max_dist = 0\n",
        "    for wordA in setA:\n",
        "      max_dist = 0\n",
        "      for wordB in setB:\n",
        "        if wordA not in list(setA.intersection(setB)) and wordB not in list(setA.intersection(setB)):\n",
        "          dist = levenshtein_distance_metric(wordA, wordB)\n",
        "          if dist > max_dist and dist > 0.75:\n",
        "            max_dist = dist\n",
        "      num_intersection = num_intersection+max_dist\n",
        "    min_len = len(setA)\n",
        "    if(min_len > len(setB)):\n",
        "      min_len = len(setB)\n",
        "    if min_len == 0:\n",
        "      return 0.0\n",
        "    return float(num_intersection)/min_len\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "def jaccard_levenshtein(A, B):\n",
        "  if A is None or B is None or len(A) == 0 or len(B) == 0:\n",
        "    return 0.0\n",
        "  setA = tokenize(A)\n",
        "  setB = tokenize(B)\n",
        "  num_intersection = len(setA.intersection(setB))\n",
        "  for wordA in setA:\n",
        "    max_dist = 0\n",
        "    for wordB in setB:\n",
        "      if wordB not in setA.intersection(setB):\n",
        "        dist = levenshtein_distance_metric(wordA, wordB)\n",
        "        if dist > max_dist and max_dist > 0.75:\n",
        "          max_dist = dist\n",
        "      num_intersection = num_intersection + max_dist\n",
        "  return float(num_intersection)/(len(setA) + len(setB) - num_intersection)\n",
        "\n",
        "def levenshtein_distance_metric(A, B):\n",
        "  try:\n",
        "      if A is None or B is None or len(A)==0 or len(B)==0:\n",
        "          return 0.0\n",
        "      max_len = len(A)\n",
        "      if max_len < len(B):\n",
        "        max_len = len(B)\n",
        "      return 1 - float(levenshtein_distance(A, B))/max_len\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "def jaro_winkler_metric(A,B):\n",
        "    if A is None or B is None or len(A)==0 or len(B)==0:\n",
        "        return 0.0\n",
        "                  \n",
        "    return float(jaro_winkler(A.lower(),B.lower()))\n",
        "\n",
        "def exact_metric(A,B):\n",
        "    if A is None or B is None:\n",
        "        return 0.0\n",
        "    if A==B:\n",
        "        return 0.25\n",
        "    else:\n",
        "        return 0.0\n",
        "\n",
        "def overlap_levenshtein(A, B):\n",
        "  try:    \n",
        "    if A is None or B is None or len(A) == 0 or len(B) == 0:\n",
        "      return 0.0\n",
        "    setA = set(tokenize(A))\n",
        "    setB = set(tokenize(B))\n",
        "    num_intersection = len(setA.intersection(setB))\n",
        "    intersected_word = list(setA.intersection(setB))\n",
        "    max_dist = 0\n",
        "    for wordA in setA:\n",
        "      max_dist = 0\n",
        "      for wordB in setB:\n",
        "        if wordA not in intersected_word and wordB not in intersected_word:\n",
        "          dist = levenshtein_distance_metric(wordA, wordB)\n",
        "          if dist > max_dist and dist > 0.75:\n",
        "            max_dist = dist\n",
        "      num_intersection = num_intersection+max_dist\n",
        "    min_len = len(setA)\n",
        "    if(min_len > len(setB)):\n",
        "      min_len = len(setB)\n",
        "    if min_len == 0:\n",
        "      return 0.0\n",
        "    return float(num_intersection)/min_len\n",
        "  except Exception:\n",
        "    pass\n",
        "\n",
        "def soundex_metric(A, B):\n",
        "    try:\n",
        "      if A is None or B is None or A is \"\" or B is \"\" or len(A) == 0 or len(B) == 0:\n",
        "          return 0.0\n",
        "      \n",
        "      setA = tokenize(A)\n",
        "      setB = tokenize(B)\n",
        "      soundexA = set()\n",
        "      soundexB = set()\n",
        "      for wordA in setA:\n",
        "        soundexA.add(soundex(wordA))\n",
        "      \n",
        "      for wordB in setB:\n",
        "        soundexB.add(soundex(wordB))\n",
        "        \n",
        "      intersection = 0\n",
        "      for wordA in soundexA:\n",
        "        if wordA in soundexB:\n",
        "          intersection = intersection+1\n",
        "    \n",
        "      min_len = len(soundexA)\n",
        "      if min_len > len(soundexB):\n",
        "        min_len = len(soundexB)\n",
        "      if min_len == 0.0:\n",
        "        return 0.0\n",
        "      \n",
        "      return float(intersection)/(min_len)\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "def swapped_attribute(fir, sec, pair_of_header, func, header_index):\n",
        "  combinations = [(0,0),(0,1),(1,0),(1,1)]\n",
        "  metric_values = list()\n",
        "  for each_combination in combinations:\n",
        "    metric_values.append(func(fir[header_index[pair_of_header[each_combination[0]]]], sec[header_index[pair_of_header[each_combination[1]]]]))\n",
        "  swp_1 = metric_values[0]+metric_values[3]\n",
        "  swp_2 = metric_values[1]+metric_values[2]\n",
        "  if swp_1>swp_2:\n",
        "    return metric_values[0], metric_values[3]\n",
        "  else:\n",
        "    return metric_values[1],metric_values[2]\n",
        "\n",
        "def applyAlgorithms(dataframe, algorithms, column_list):\n",
        "  counter = 0\n",
        "  column_length = len(column_list)\n",
        "  for key,values_list in algorithms.items():\n",
        "    values_list.sort()\n",
        "    if key == LEVENSHTEIN:\n",
        "      for value in values_list:\n",
        "        column_name_list = [column_name for column_name in column_list if value in column_name]\n",
        "        col_name = value+\"_\"+str(column_length+counter)\n",
        "        df[col_name] = df.apply(lambda row: levenshtein_distance_metric(row['%s'%column_name_list[0]], row['%s'%column_name_list[1]]),axis=1)\n",
        "        # dataframe = dataframe.withColumn(\"feature_\"+str(column_length+counter), apply_levenshtein(column_name_list[0], column_name_list[1]))\n",
        "        counter = counter+1\n",
        "        \n",
        "    elif key == SOUNDEX:\n",
        "      for value in values_list:\n",
        "        column_name_list = [column_name for column_name in column_list if value in column_name]\n",
        "        col_name = value+\"_\"+str(column_length+counter)\n",
        "        df[col_name] = df.apply(lambda row: soundex_metric(row['%s'%column_name_list[0]], row['%s'%column_name_list[1]]),axis=1)\n",
        "        # dataframe = dataframe.withColumn(\"feature_\"+str(column_length+counter), apply_soundex(column_name_list[0], column_name_list[1]))\n",
        "        counter = counter+1\n",
        "        \n",
        "    elif key == OVERLAP:\n",
        "      for value in values_list:\n",
        "        column_name_list = [column_name for column_name in column_list if value in column_name]\n",
        "        col_name = value+\"_\"+str(column_length+counter)\n",
        "        df[col_name] = df.apply(lambda row: overlap(row['%s'%column_name_list[0]], row['%s'%column_name_list[1]]),axis=1)\n",
        "        # dataframe = dataframe.withColumn(\"feature_\"+str(column_length+counter), apply_overlap(column_name_list[0], column_name_list[1]))\n",
        "        counter = counter+1\n",
        "        \n",
        "    elif key == OVERLAPLEVENSHTEIN:\n",
        "      for value in values_list:\n",
        "        column_name_list = [column_name for column_name in column_list if value in column_name]\n",
        "        col_name = value+\"_\"+str(column_length+counter)\n",
        "        df[col_name] = df.apply(lambda row: overlap_levenshtein(row['%s'%column_name_list[0]], row['%s'%column_name_list[1]]),axis=1)\n",
        "        # dataframe = dataframe.withColumn(\"feature_\"+str(column_length+counter), apply_overlap_levenshtein(column_name_list[0], column_name_list[1]))\n",
        "        counter = counter+1\n",
        "        \n",
        "    elif key == JACCARD:\n",
        "      for value in values_list:\n",
        "        column_name_list = [column_name for column_name in column_list if value in column_name]\n",
        "        col_name = value+\"_\"+str(column_length+counter)\n",
        "        df[col_name] = df.apply(lambda row: jaccard(row['%s'%column_name_list[0]], row['%s'%column_name_list[1]]),axis=1)\n",
        "        # dataframe = dataframe.withColumn(\"feature_\"+str(column_length+counter), apply_jaccard(column_name_list[0], column_name_list[1]))\n",
        "        counter = counter+1\n",
        "        \n",
        "    if key == JACCARDLEVENSHTEIN:\n",
        "      for value in values_list:\n",
        "        column_name_list = [column_name for column_name in column_list if value in column_name]\n",
        "        col_name = value+\"_\"+str(column_length+counter)\n",
        "        df[col_name] = df.apply(lambda row: jaccard_levenshtein(row['%s'%column_name_list[0]], row['%s'%column_name_list[1]]),axis=1)\n",
        "        # dataframe = dataframe.withColumn(\"feature_\"+str(column_length+counter), apply_jaccard_levenshtein(column_name_list[0], column_name_list[1]))\n",
        "        counter = counter+1\n",
        "        \n",
        "  return dataframe, counter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrUbJOexWXI2",
        "colab_type": "text"
      },
      "source": [
        "# **Processing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phUPAWpBZ4vz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bb = applyAlgorithms(dataframe=df,algorithms=algos,column_list=cols)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqOTYtTWaeza",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = bb[0]\n",
        "train_df = df[['system_address_14', 'system_alternatename_15', 'system_businessname_16', 'system_phonenumber_17', 'system_universalbusinessnumber_18', 'system_alternatename_19', 'system_businessname_20','LABEL']]\n",
        "target = train_df.pop(\"LABEL\")\n",
        "dataset = tf.data.Dataset.from_tensor_slices((train_df.values, target.values))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6eM69BMi6_l",
        "colab_type": "text"
      },
      "source": [
        "# **Model Building**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQozkWS90jri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import tensorboardcolab as tbc\n",
        "from tensorboardcolab import TensorBoardColabCallback"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "91910576-3f5d-42ee-9c2f-2bab72cc1804",
        "id": "2eYOPDpB0dKi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "train_dataset = dataset.shuffle(len(df)).batch(1)\n",
        "tboard = tbc.TensorBoardColab()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "http://63d67953.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6F0fTVy04nl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "cf8365f7-121f-4657-a4fc-bd64f13317ed"
      },
      "source": [
        "def get_compiled_model():\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "\n",
        "  tbCallBack = TensorBoard(log_dir='./log', histogram_freq=1,\n",
        "                         write_graph=True,\n",
        "                         write_grads=True,\n",
        "                         write_images=True)\n",
        "\n",
        "\n",
        "  model.compile(optimizer='adam',\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'],\n",
        "                verbose =1,\n",
        "                callbacks=[TensorBoardColabCallback(tboard)])\n",
        "  return model\n",
        "model = get_compiled_model()\n",
        "history = model.fit(train_dataset, epochs=5)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, log_dir, histogram_freq, batch_size, write_graph, write_grads, write_images, embeddings_freq, embeddings_layer_names, embeddings_metadata, embeddings_data, update_freq)\u001b[0m\n\u001b[1;32m   1017\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1018\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplugins\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprojector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1019\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.contrib'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-a2aec548b9ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m                 callbacks=[TensorBoardColabCallback(tboard)])\n\u001b[1;32m     22\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_compiled_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-a2aec548b9ea>\u001b[0m in \u001b[0;36mget_compiled_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                          \u001b[0mwrite_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                          \u001b[0mwrite_grads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                          write_images=True)\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, log_dir, histogram_freq, batch_size, write_graph, write_grads, write_images, embeddings_freq, embeddings_layer_names, embeddings_metadata, embeddings_data, update_freq)\u001b[0m\n\u001b[1;32m   1018\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplugins\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprojector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m             raise ImportError('You need the TensorFlow module installed to '\n\u001b[0m\u001b[1;32m   1021\u001b[0m                               'use TensorBoard.')\n\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: You need the TensorFlow module installed to use TensorBoard.",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f66NhxBskjSy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "loss = history.history['acc']\n",
        "val_loss = history.history['val_acc']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "plt.figure(figsize=(17,10))\n",
        "plt.plot(epochs, loss, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc=0)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21Ru5CEBjBVi",
        "colab_type": "text"
      },
      "source": [
        "# **TensorFlow Dataset API for GPU and TPU processing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksFTnFuOgxw0",
        "colab_type": "text"
      },
      "source": [
        "<br> <br> <br> \n",
        "**Load Data frame and Make Tensor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8sGa54nmXn0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_slices = tf.data.Dataset.from_tensor_slices(dict(df))\n",
        "for feature_batch in data_slices.take(1):\n",
        "  for key, value in feature_batch.items():\n",
        "    print(\"  {!r:20s}: {}\".format(key, value))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yBAZjIwgHiQ",
        "colab_type": "text"
      },
      "source": [
        "**Load CSV data and create a tensor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFAyS6LtyS_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_dataset(file_path, **kwargs):\n",
        "  dataset = tf.data.experimental.make_csv_dataset(\n",
        "      file_path,\n",
        "      batch_size=5, # Artificially small to make examples easier to show.\n",
        "      label_name=\"LABEL\",\n",
        "      na_value=\"?\",\n",
        "      num_epochs=1,\n",
        "      ignore_errors=True, \n",
        "      **kwargs)\n",
        "  return dataset\n",
        "\n",
        "raw_train_data = get_dataset(\"/content/BigData.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMC-U-vHdlLZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_batch(dataset):\n",
        "  for batch, label in dataset.take(1):\n",
        "    for key, value in batch.items():\n",
        "      print(\"{:20s}: {}\".format(key,value.numpy()))\n",
        "\n",
        "show_batch(raw_train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgxUtIKshrb8",
        "colab_type": "text"
      },
      "source": [
        "<br><br><br><br><br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iROIM3VBZRiC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processed_df, counter = applyAlgorithms(dataframe=df,algorithms=algos,column_list=cols)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}