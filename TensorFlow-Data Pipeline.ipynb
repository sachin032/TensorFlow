{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T07:23:39.950144Z",
     "start_time": "2020-02-07T07:23:39.945586Z"
    }
   },
   "source": [
    "<img src=\"Pipeline1.jpg\" style=\"width:100\"  alt=\"TEnsorflow Data Input Pipeline\" title=\"Data Input Pipeline\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                       **Data Input Pipeline**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In any Modeling Pipeline there are basaic generic steps to be followed.<br>\n",
    "1: Get the data.<br>\n",
    "2: EDA<br>\n",
    "3: Preprocessing and Feature engineering<br>\n",
    "4: Input the preprocessed and clean data to Modeling mechanism.<br>\n",
    "5: Analyse the training output.<br>**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To get started we need to get the dataset from any data source.<br>\n",
    "Datasources can be located at:<br>\n",
    "    1:in-memeory<br>\n",
    "    2:Locally stored<br>\n",
    "    3:Remotely deployed.<br>**\n",
    "    \n",
    "***    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data can have varying formats as well irrelavant of it's storage location:<br>\n",
    "1: *Structured*  \n",
    "2: *Unstructured*  \n",
    "3: *Semi-Structured***\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the size of the data we start preprocessing and model training and testing on a specific architecture\n",
    "For example if we have data with certain format and big size, we need to access it and utilize it with help of distributed manner.<br><br>\n",
    "Based on the underlying hardware capabilities we can take advantage of underlying processing power for faster processing.<br>For an example we can run computation on **GPUs** or **TPUs** instead of assigning the processing to **CPUs**\n",
    "\n",
    "&#129409;\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T07:35:36.596613Z",
     "start_time": "2020-02-07T07:35:34.864040Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ways to create _Tensorflow Dataset_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T07:35:36.630278Z",
     "start_time": "2020-02-07T07:35:36.598585Z"
    }
   },
   "outputs": [],
   "source": [
    "#Using from_tensor_slices(tensor)\n",
    "dataset = tf.data.Dataset.from_tensor_slices([8, 3, 0, 8, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T07:35:36.639898Z",
     "start_time": "2020-02-07T07:35:36.634066Z"
    }
   },
   "outputs": [],
   "source": [
    "#Using from_tensors\n",
    "dataset = tf.data.Dataset.from_tensors([1.2,3.4,5.6,7.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T07:35:36.675723Z",
     "start_time": "2020-02-07T07:35:36.641466Z"
    }
   },
   "outputs": [],
   "source": [
    "#Using from_generator\n",
    "def gen():\n",
    "    for i in itertools.count(1):\n",
    "        yield (i, [1] * i)\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(gen, (tf.int64, tf.int64), (tf.TensorShape([]), tf.TensorShape([None])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T07:35:36.683181Z",
     "start_time": "2020-02-07T07:35:36.677604Z"
    }
   },
   "outputs": [],
   "source": [
    "#using tf.data.Dataset.zip((datasets seperated by comma))\n",
    "dataset_1 = tf.data.Dataset.from_tensors([1,2,3])\n",
    "dataset_2 = tf.data.Dataset.from_tensors([4,9,2])\n",
    "\n",
    "dataset_3 = tf.data.Dataset.zip((dataset_1,dataset_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T07:35:36.826146Z",
     "start_time": "2020-02-07T07:35:36.685998Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sachin/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/experimental/ops/readers.py:521: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n"
     ]
    }
   ],
   "source": [
    "#Using experimental API\n",
    "csv_file_path = \"/home/sachin/Documents/BigData.csv\"\n",
    "dataset = tf.data.experimental.make_csv_dataset(\n",
    "      csv_file_path,\n",
    "      batch_size=5, # Artificially small to make examples easier to show.\n",
    "      label_name=\"LABEL\",\n",
    "      na_value=\"?\",\n",
    "      num_epochs=1,\n",
    "      ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T07:35:36.862771Z",
     "start_time": "2020-02-07T07:35:36.828697Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Using TFRecordDataset\n",
    "fsns_test_file = tf.keras.utils.get_file(\"fsns.tfrec\", \"https://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/testdata/fsns-00000-of-00001\")\n",
    "dataset = tf.data.TFRecordDataset(filenames=[fsns_test_file])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=green>Ways to iterate over the Tensorflow Dataset into Batches</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T08:25:57.605461Z",
     "start_time": "2020-02-07T08:25:57.560054Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Batch number ::  1\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "\n",
      "\n",
      "Batch number ::  2\n",
      "[20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39]\n",
      "\n",
      "\n",
      "Batch number ::  3\n",
      "[40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59]\n",
      "\n",
      "\n",
      "Batch number ::  4\n",
      "[60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79]\n",
      "\n",
      "\n",
      "Batch number ::  5\n",
      "[80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99]\n",
      "\n",
      "\n",
      "Batch number ::  6\n",
      "[100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117\n",
      " 118 119]\n",
      "\n",
      "\n",
      "Batch number ::  7\n",
      "[120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137\n",
      " 138 139]\n",
      "\n",
      "\n",
      "Batch number ::  8\n",
      "[140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157\n",
      " 158 159]\n",
      "\n",
      "\n",
      "Batch number ::  9\n",
      "[160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177\n",
      " 178 179]\n",
      "\n",
      "\n",
      "Batch number ::  10\n",
      "[180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199]\n",
      "\n",
      "\n",
      "Batch number ::  11\n",
      "[200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217\n",
      " 218 219]\n",
      "\n",
      "\n",
      "Batch number ::  12\n",
      "[220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237\n",
      " 238 239]\n",
      "\n",
      "\n",
      "Batch number ::  13\n",
      "[240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257\n",
      " 258 259]\n",
      "\n",
      "\n",
      "Batch number ::  14\n",
      "[260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277\n",
      " 278 279]\n",
      "\n",
      "\n",
      "Batch number ::  15\n",
      "[280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297\n",
      " 298 299]\n",
      "\n",
      "\n",
      "Batch number ::  16\n",
      "[300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317\n",
      " 318 319]\n",
      "\n",
      "\n",
      "Batch number ::  17\n",
      "[320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337\n",
      " 338 339]\n",
      "\n",
      "\n",
      "Batch number ::  18\n",
      "[340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357\n",
      " 358 359]\n",
      "\n",
      "\n",
      "Batch number ::  19\n",
      "[360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379]\n",
      "\n",
      "\n",
      "Batch number ::  20\n",
      "[380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397\n",
      " 398 399]\n",
      "\n",
      "\n",
      "Batch number ::  21\n",
      "[400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417\n",
      " 418 419]\n",
      "\n",
      "\n",
      "Batch number ::  22\n",
      "[420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437\n",
      " 438 439]\n",
      "\n",
      "\n",
      "Batch number ::  23\n",
      "[440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457\n",
      " 458 459]\n",
      "\n",
      "\n",
      "Batch number ::  24\n",
      "[460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477\n",
      " 478 479]\n",
      "\n",
      "\n",
      "Batch number ::  25\n",
      "[480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497\n",
      " 498 499]\n",
      "\n",
      "\n",
      "Batch number ::  26\n",
      "[500 501]\n"
     ]
    }
   ],
   "source": [
    "dataset_1 = tf.data.Dataset.range(502)\n",
    "batch_dataset = dataset_1.batch(20)\n",
    "\n",
    "i=1\n",
    "for batch in batch_dataset:\n",
    "    print(\"\\n\\nBatch number :: \",i)\n",
    "    i+=1    \n",
    "    print(batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T08:28:28.422669Z",
     "start_time": "2020-02-07T08:28:28.411328Z"
    }
   },
   "source": [
    "**Applying custom function to Tensorflow dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensors(tf.random.uniform((1000)))\n",
    "\n",
    "for \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "212px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
