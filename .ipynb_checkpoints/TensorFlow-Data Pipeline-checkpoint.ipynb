{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **To improve input pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T04:55:43.773654Z",
     "start_time": "2020-02-07T04:55:40.715066Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In any Modeling Pipeline there are basaic generic steps to be followed.<br>\n",
    "1: Get the data.<br>\n",
    "2: EDA<br>\n",
    "3: Preprocessing and Feature engineering<br>\n",
    "4: Input the preprocessed and clean data to Modeling mechanism.<br>\n",
    "5: Analyse the training output.<br>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To get started we need to get the dataset from any data source.<br>\n",
    "Datasources can be located at:<br>\n",
    "    1:in-memeory<br>\n",
    "    2:Locally stored<br>\n",
    "    3:Remotely deployed.<br>**\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data can have varying formats as well irrelavant of it's storage location:<br>\n",
    "1: Structured<br>\n",
    "2: Unstructured<br>\n",
    "3: Semi-Structured<br>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ways to create Tensorflow Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T04:55:43.840899Z",
     "start_time": "2020-02-07T04:55:43.784578Z"
    }
   },
   "outputs": [],
   "source": [
    "#Using from_tensor_slices(tensor)\n",
    "dataset = tf.data.Dataset.from_tensor_slices([8, 3, 0, 8, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T04:58:30.137011Z",
     "start_time": "2020-02-07T04:58:30.131391Z"
    }
   },
   "outputs": [],
   "source": [
    "#Using from_tensors\n",
    "dataset = tf.data.Dataset.from_tensors([1.2,3.4,5.6,7.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T05:01:01.764231Z",
     "start_time": "2020-02-07T05:01:01.723532Z"
    }
   },
   "outputs": [],
   "source": [
    "#Using from_generator\n",
    "def gen():\n",
    "    for i in itertools.count(1):\n",
    "        yield (i, [1] * i)\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(gen, (tf.int64, tf.int64), (tf.TensorShape([]), tf.TensorShape([None])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T05:04:33.229169Z",
     "start_time": "2020-02-07T05:04:33.214315Z"
    }
   },
   "outputs": [],
   "source": [
    "#using tf.data.Dataset.zip((datasets seperated by comma))\n",
    "dataset_1 = tf.data.Dataset.from_tensors([1,2,3])\n",
    "dataset_2 = tf.data.Dataset.from_tensors([4,9,2])\n",
    "\n",
    "dataset_3 = tf.data.Dataset.zip((dataset_1,dataset_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T05:22:29.604221Z",
     "start_time": "2020-02-07T05:22:29.473039Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sachin/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/experimental/ops/readers.py:521: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n"
     ]
    }
   ],
   "source": [
    "#Using experimental API\n",
    "csv_file_path = \"/home/sachin/Documents/BigData.csv\"\n",
    "dataset = tf.data.experimental.make_csv_dataset(\n",
    "      csv_file_path,\n",
    "      batch_size=5, # Artificially small to make examples easier to show.\n",
    "      label_name=\"LABEL\",\n",
    "      na_value=\"?\",\n",
    "      num_epochs=1,\n",
    "      ignore_errors=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T06:58:22.360596Z",
     "start_time": "2020-02-07T06:58:17.623028Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/testdata/fsns-00000-of-00001\n",
      "7905280/7904079 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "#Using TFRecordDataset\n",
    "fsns_test_file = tf.keras.utils.get_file(\"fsns.tfrec\", \"https://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/testdata/fsns-00000-of-00001\")\n",
    "dataset = tf.data.TFRecordDataset(filenames=[fsns_test_file])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ways to iterate over the Tensorflow Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T05:31:52.672606Z",
     "start_time": "2020-02-07T05:31:52.657927Z"
    }
   },
   "outputs": [],
   "source": [
    "#OneShot Iterator\n",
    "#Pros : \n",
    "#Cons :\n",
    "import numpy as np\n",
    "\n",
    "x = np.random.sample((100,2))\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "212px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
