{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sachin032/Tensorflow/blob/master/Tensorflow_MDM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1hGUpEBSivzE"
   },
   "source": [
    "# **Download Required third party dependancies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t_5md7Pdg8_b"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    !pip install -q tf-nightly fuzzywuzzy metaphone whoosh jellyfish\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "dvn-OT4wsu5u",
    "outputId": "b3d8163f-7b2c-4804-8075-59138740500c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-02-05 09:56:07--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
      "Resolving bin.equinox.io (bin.equinox.io)... 52.204.140.35, 34.238.36.128, 52.7.241.210, ...\n",
      "Connecting to bin.equinox.io (bin.equinox.io)|52.204.140.35|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13773305 (13M) [application/octet-stream]\n",
      "Saving to: ‘ngrok-stable-linux-amd64.zip.1’\n",
      "\n",
      "\r",
      "          ngrok-sta   0%[                    ]       0  --.-KB/s               \r",
      "ngrok-stable-linux- 100%[===================>]  13.13M  85.1MB/s    in 0.2s    \n",
      "\n",
      "2020-02-05 09:56:07 (85.1 MB/s) - ‘ngrok-stable-linux-amd64.zip.1’ saved [13773305/13773305]\n",
      "\n",
      "Archive:  ngrok-stable-linux-amd64.zip\n",
      "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
      "  inflating: ngrok                   \n"
     ]
    }
   ],
   "source": [
    "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "!unzip ngrok-stable-linux-amd64.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "n5oqVv8Esytl",
    "outputId": "f5c76152-e2b0-492f-b979-e2d302fc1157"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboardcolab in /usr/local/lib/python3.6/dist-packages (0.0.22)\n",
      "--2020-02-05 09:56:59--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
      "Resolving bin.equinox.io (bin.equinox.io)... 52.204.140.35, 34.238.36.128, 52.7.241.210, ...\n",
      "Connecting to bin.equinox.io (bin.equinox.io)|52.204.140.35|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13773305 (13M) [application/octet-stream]\n",
      "Saving to: ‘ngrok-stable-linux-amd64.zip.2’\n",
      "\n",
      "ngrok-stable-linux- 100%[===================>]  13.13M  77.3MB/s    in 0.2s    \n",
      "\n",
      "2020-02-05 09:56:59 (77.3 MB/s) - ‘ngrok-stable-linux-amd64.zip.2’ saved [13773305/13773305]\n",
      "\n",
      "Archive:  ngrok-stable-linux-amd64.zip\n",
      "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
      "  inflating: ngrok                   \n",
      "https://63d67953.ngrok.io\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboardcolab\n",
    "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "!unzip ngrok-stable-linux-amd64.zip\n",
    "\n",
    "\n",
    "\n",
    "LOG_DIR = './log'\n",
    "get_ipython().system_raw('tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'.format(LOG_DIR))\n",
    "get_ipython().system_raw('./ngrok http 6006 &')\n",
    "\n",
    "! curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "60hBMzpOiCKW"
   },
   "source": [
    "# **Load the Data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "2gW8M8vBRdoB",
    "outputId": "9b8860e5-e142-453d-9f38-0ce8322dbdd6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-cdd49cba-56bd-4cb7-9023-fecc92df2b72\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-cdd49cba-56bd-4cb7-9023-fecc92df2b72\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving BigData.csv to BigData (3).csv\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "data =  files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V31nPf4uiI5h"
   },
   "source": [
    "# **Load Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ui_V6U-x_VG5",
    "outputId": "679a7557-4ef9-491c-abda-a009a3b11d5e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import tensorflow as tf\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pathlib\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer,hashing_trick,one_hot,text_to_word_sequence\n",
    "\n",
    "from jellyfish import jaro_winkler, levenshtein_distance, soundex\n",
    "from whoosh.analysis import StandardAnalyzer\n",
    "from metaphone import doublemetaphone\n",
    "from fuzzywuzzy import fuzz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9owgBBGNiRD1"
   },
   "source": [
    "# **Taxonomy Declaration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RVMqM1EqglR3"
   },
   "outputs": [],
   "source": [
    "ACCOUNT_NAME = 'accountName'\n",
    "ACCOUNT_KEY = 'accountKey'\n",
    "CONTAINER_NAME = 'containerName'\n",
    "ANALYTICS_EXTERNAL = 'analytics-external'\n",
    "BLOB_END_SUFFIX = \".blob.core.windows.net\"\n",
    "BLOB_PREFIX = \"fs.azure.account.key.\"\n",
    "ANALYTICS_INTERNAL = 'analytics-internal'\n",
    "LEVENSHTEIN = 'levenshtein'\n",
    "SOUNDEX = 'soundex'\n",
    "OVERLAP = 'overlap'\n",
    "OVERLAPLEVENSHTEIN = 'overlapLevenshtein'\n",
    "JACCARD = 'jaccard'\n",
    "JACCARDLEVENSHTEIN = 'jaccardLevenshtein'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xg_tVH_jTgdb"
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4)\n",
    "df = pd.read_csv(io.StringIO(data['BigData.csv'].decode('utf-8')))\n",
    "df['system_universalbusinessnumber_11'] = df[\n",
    "    'system_universalbusinessnumber_11'].astype(str)\n",
    "df['system_universalbusinessnumber_12'] = df[\n",
    "    'system_universalbusinessnumber_11'].astype(str)\n",
    "df = df.drop([\n",
    "    'feature_14', 'feature_15', 'feature_16', 'feature_17', 'feature_18',\n",
    "    'feature_19', 'feature_20', 'feature_21'\n",
    "],\n",
    "             axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SuKPe0Q4XbSt"
   },
   "outputs": [],
   "source": [
    "algos = {\n",
    "    'jaro_winkler': [],\n",
    "    'exact_match': [],\n",
    "    'overlapLevenshtein': [\n",
    "        'system_businessname', 'system_address', 'system_alternatename',\n",
    "        'system_phonenumber', 'system_universalbusinessnumber'\n",
    "    ],\n",
    "    'soundex': ['system_businessname', 'system_alternatename']\n",
    "}\n",
    "\n",
    "cols = [\n",
    "    'id', 'system_businessname_1', 'system_businessname_2',\n",
    "    'system_alternatename_3', 'system_alternatename_4', 'system_address_5',\n",
    "    'system_address_6', 'system_phonenumber_7', 'system_phonenumber_8',\n",
    "    'system_dateofinception_9', 'system_dateofinception_10',\n",
    "    'system_universalbusinessnumber_11', 'system_universalbusinessnumber_12',\n",
    "    'LABEL'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YiJ8kv8SinXC"
   },
   "source": [
    "# **Data Preprocessing Steps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QtX1qFZ3Wkhr"
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    analyzer = StandardAnalyzer()\n",
    "    return [t.text for t in analyzer(text)]\n",
    "\n",
    "\n",
    "def overlap(A, B):\n",
    "    try:\n",
    "        if A is None or B is None or len(A) == 0 or len(B) == 0:\n",
    "            return 0.0\n",
    "        setA = tokenize(A)\n",
    "        setB = tokenize(B)\n",
    "        num_intersection = setA.intersection(setB)\n",
    "        min_len = len(setA)\n",
    "        if (min_len > len(setB)):\n",
    "            min_len = len(setB)\n",
    "        return float(intersection) / min_len\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def jaccard(A, B):\n",
    "    if A is None or B is None or len(A) == 0 or len(B) == 0:\n",
    "        return 0.0\n",
    "    setA = tokenize(A)\n",
    "    setB = tokenize(B)\n",
    "    num_intersection = setA.intersection(setB)\n",
    "    return float(intersection) / (len(setA) + len(setB) - intersection)\n",
    "\n",
    "\n",
    "def overlap_levenshtein(A, B):\n",
    "    try:\n",
    "        if A is None or B is None or len(A) == 0 or len(B) == 0:\n",
    "            return 0.0\n",
    "        setA = set(tokenize(A))\n",
    "        setB = set(tokenize(B))\n",
    "        num_intersection = len(setA.intersection(setB))\n",
    "        max_dist = 0\n",
    "        for wordA in setA:\n",
    "            max_dist = 0\n",
    "            for wordB in setB:\n",
    "                if wordA not in list(\n",
    "                        setA.intersection(setB)) and wordB not in list(\n",
    "                            setA.intersection(setB)):\n",
    "                    dist = levenshtein_distance_metric(wordA, wordB)\n",
    "                    if dist > max_dist and dist > 0.75:\n",
    "                        max_dist = dist\n",
    "            num_intersection = num_intersection + max_dist\n",
    "        min_len = len(setA)\n",
    "        if (min_len > len(setB)):\n",
    "            min_len = len(setB)\n",
    "        if min_len == 0:\n",
    "            return 0.0\n",
    "        return float(num_intersection) / min_len\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def jaccard_levenshtein(A, B):\n",
    "    if A is None or B is None or len(A) == 0 or len(B) == 0:\n",
    "        return 0.0\n",
    "    setA = tokenize(A)\n",
    "    setB = tokenize(B)\n",
    "    num_intersection = len(setA.intersection(setB))\n",
    "    for wordA in setA:\n",
    "        max_dist = 0\n",
    "        for wordB in setB:\n",
    "            if wordB not in setA.intersection(setB):\n",
    "                dist = levenshtein_distance_metric(wordA, wordB)\n",
    "                if dist > max_dist and max_dist > 0.75:\n",
    "                    max_dist = dist\n",
    "            num_intersection = num_intersection + max_dist\n",
    "    return float(num_intersection) / (len(setA) + len(setB) - num_intersection)\n",
    "\n",
    "\n",
    "def levenshtein_distance_metric(A, B):\n",
    "    try:\n",
    "        if A is None or B is None or len(A) == 0 or len(B) == 0:\n",
    "            return 0.0\n",
    "        max_len = len(A)\n",
    "        if max_len < len(B):\n",
    "            max_len = len(B)\n",
    "        return 1 - float(levenshtein_distance(A, B)) / max_len\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def jaro_winkler_metric(A, B):\n",
    "    if A is None or B is None or len(A) == 0 or len(B) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return float(jaro_winkler(A.lower(), B.lower()))\n",
    "\n",
    "\n",
    "def exact_metric(A, B):\n",
    "    if A is None or B is None:\n",
    "        return 0.0\n",
    "    if A == B:\n",
    "        return 0.25\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def overlap_levenshtein(A, B):\n",
    "    try:\n",
    "        if A is None or B is None or len(A) == 0 or len(B) == 0:\n",
    "            return 0.0\n",
    "        setA = set(tokenize(A))\n",
    "        setB = set(tokenize(B))\n",
    "        num_intersection = len(setA.intersection(setB))\n",
    "        intersected_word = list(setA.intersection(setB))\n",
    "        max_dist = 0\n",
    "        for wordA in setA:\n",
    "            max_dist = 0\n",
    "            for wordB in setB:\n",
    "                if wordA not in intersected_word and wordB not in intersected_word:\n",
    "                    dist = levenshtein_distance_metric(wordA, wordB)\n",
    "                    if dist > max_dist and dist > 0.75:\n",
    "                        max_dist = dist\n",
    "            num_intersection = num_intersection + max_dist\n",
    "        min_len = len(setA)\n",
    "        if (min_len > len(setB)):\n",
    "            min_len = len(setB)\n",
    "        if min_len == 0:\n",
    "            return 0.0\n",
    "        return float(num_intersection) / min_len\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "def soundex_metric(A, B):\n",
    "    try:\n",
    "        if A is None or B is None or A is \"\" or B is \"\" or len(A) == 0 or len(\n",
    "                B) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        setA = tokenize(A)\n",
    "        setB = tokenize(B)\n",
    "        soundexA = set()\n",
    "        soundexB = set()\n",
    "        for wordA in setA:\n",
    "            soundexA.add(soundex(wordA))\n",
    "\n",
    "        for wordB in setB:\n",
    "            soundexB.add(soundex(wordB))\n",
    "\n",
    "        intersection = 0\n",
    "        for wordA in soundexA:\n",
    "            if wordA in soundexB:\n",
    "                intersection = intersection + 1\n",
    "\n",
    "        min_len = len(soundexA)\n",
    "        if min_len > len(soundexB):\n",
    "            min_len = len(soundexB)\n",
    "        if min_len == 0.0:\n",
    "            return 0.0\n",
    "\n",
    "        return float(intersection) / (min_len)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def swapped_attribute(fir, sec, pair_of_header, func, header_index):\n",
    "    combinations = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "    metric_values = list()\n",
    "    for each_combination in combinations:\n",
    "        metric_values.append(\n",
    "            func(fir[header_index[pair_of_header[each_combination[0]]]],\n",
    "                 sec[header_index[pair_of_header[each_combination[1]]]]))\n",
    "    swp_1 = metric_values[0] + metric_values[3]\n",
    "    swp_2 = metric_values[1] + metric_values[2]\n",
    "    if swp_1 > swp_2:\n",
    "        return metric_values[0], metric_values[3]\n",
    "    else:\n",
    "        return metric_values[1], metric_values[2]\n",
    "\n",
    "\n",
    "def applyAlgorithms(dataframe, algorithms, column_list):\n",
    "    counter = 0\n",
    "    column_length = len(column_list)\n",
    "    for key, values_list in algorithms.items():\n",
    "        values_list.sort()\n",
    "        if key == LEVENSHTEIN:\n",
    "            for value in values_list:\n",
    "                column_name_list = [\n",
    "                    column_name for column_name in column_list\n",
    "                    if value in column_name\n",
    "                ]\n",
    "                col_name = value + \"_\" + str(column_length + counter)\n",
    "                df[col_name] = df.apply(\n",
    "                    lambda row: levenshtein_distance_metric(\n",
    "                        row['%s' % column_name_list[0]], row[\n",
    "                            '%s' % column_name_list[1]]),\n",
    "                    axis=1)\n",
    "                # dataframe = dataframe.withColumn(\"feature_\"+str(column_length+counter), apply_levenshtein(column_name_list[0], column_name_list[1]))\n",
    "                counter = counter + 1\n",
    "\n",
    "        elif key == SOUNDEX:\n",
    "            for value in values_list:\n",
    "                column_name_list = [\n",
    "                    column_name for column_name in column_list\n",
    "                    if value in column_name\n",
    "                ]\n",
    "                col_name = value + \"_\" + str(column_length + counter)\n",
    "                df[col_name] = df.apply(\n",
    "                    lambda row: soundex_metric(row['%s' % column_name_list[\n",
    "                        0]], row['%s' % column_name_list[1]]),\n",
    "                    axis=1)\n",
    "                # dataframe = dataframe.withColumn(\"feature_\"+str(column_length+counter), apply_soundex(column_name_list[0], column_name_list[1]))\n",
    "                counter = counter + 1\n",
    "\n",
    "        elif key == OVERLAP:\n",
    "            for value in values_list:\n",
    "                column_name_list = [\n",
    "                    column_name for column_name in column_list\n",
    "                    if value in column_name\n",
    "                ]\n",
    "                col_name = value + \"_\" + str(column_length + counter)\n",
    "                df[col_name] = df.apply(\n",
    "                    lambda row: overlap(row['%s' % column_name_list[0]], row[\n",
    "                        '%s' % column_name_list[1]]),\n",
    "                    axis=1)\n",
    "                # dataframe = dataframe.withColumn(\"feature_\"+str(column_length+counter), apply_overlap(column_name_list[0], column_name_list[1]))\n",
    "                counter = counter + 1\n",
    "\n",
    "        elif key == OVERLAPLEVENSHTEIN:\n",
    "            for value in values_list:\n",
    "                column_name_list = [\n",
    "                    column_name for column_name in column_list\n",
    "                    if value in column_name\n",
    "                ]\n",
    "                col_name = value + \"_\" + str(column_length + counter)\n",
    "                df[col_name] = df.apply(lambda row: overlap_levenshtein(\n",
    "                    row['%s' % column_name_list[0]], row['%s' %\n",
    "                                                         column_name_list[1]]),\n",
    "                                        axis=1)\n",
    "                # dataframe = dataframe.withColumn(\"feature_\"+str(column_length+counter), apply_overlap_levenshtein(column_name_list[0], column_name_list[1]))\n",
    "                counter = counter + 1\n",
    "\n",
    "        elif key == JACCARD:\n",
    "            for value in values_list:\n",
    "                column_name_list = [\n",
    "                    column_name for column_name in column_list\n",
    "                    if value in column_name\n",
    "                ]\n",
    "                col_name = value + \"_\" + str(column_length + counter)\n",
    "                df[col_name] = df.apply(\n",
    "                    lambda row: jaccard(row['%s' % column_name_list[0]], row[\n",
    "                        '%s' % column_name_list[1]]),\n",
    "                    axis=1)\n",
    "                # dataframe = dataframe.withColumn(\"feature_\"+str(column_length+counter), apply_jaccard(column_name_list[0], column_name_list[1]))\n",
    "                counter = counter + 1\n",
    "\n",
    "        if key == JACCARDLEVENSHTEIN:\n",
    "            for value in values_list:\n",
    "                column_name_list = [\n",
    "                    column_name for column_name in column_list\n",
    "                    if value in column_name\n",
    "                ]\n",
    "                col_name = value + \"_\" + str(column_length + counter)\n",
    "                df[col_name] = df.apply(lambda row: jaccard_levenshtein(\n",
    "                    row['%s' % column_name_list[0]], row['%s' %\n",
    "                                                         column_name_list[1]]),\n",
    "                                        axis=1)\n",
    "                # dataframe = dataframe.withColumn(\"feature_\"+str(column_length+counter), apply_jaccard_levenshtein(column_name_list[0], column_name_list[1]))\n",
    "                counter = counter + 1\n",
    "\n",
    "    return dataframe, counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OrUbJOexWXI2"
   },
   "source": [
    "# **Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "phUPAWpBZ4vz"
   },
   "outputs": [],
   "source": [
    "bb = applyAlgorithms(dataframe=df, algorithms=algos, column_list=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IqOTYtTWaeza"
   },
   "outputs": [],
   "source": [
    "df = bb[0]\n",
    "train_df = df[[\n",
    "    'system_address_14', 'system_alternatename_15', 'system_businessname_16',\n",
    "    'system_phonenumber_17', 'system_universalbusinessnumber_18',\n",
    "    'system_alternatename_19', 'system_businessname_20', 'LABEL'\n",
    "]]\n",
    "target = train_df.pop(\"LABEL\")\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_df.values, target.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q6eM69BMi6_l"
   },
   "source": [
    "# **Model Building**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zQozkWS90jri"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorboardcolab as tbc\n",
    "from tensorboardcolab import TensorBoardColabCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "2eYOPDpB0dKi",
    "outputId": "91910576-3f5d-42ee-9c2f-2bab72cc1804"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wait for 8 seconds...\n",
      "TensorBoard link:\n",
      "http://63d67953.ngrok.io\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset.shuffle(len(df)).batch(1)\n",
    "tboard = tbc.TensorBoardColab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Y6F0fTVy04nl",
    "outputId": "cf8365f7-121f-4657-a4fc-bd64f13317ed"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, log_dir, histogram_freq, batch_size, write_graph, write_grads, write_images, embeddings_freq, embeddings_layer_names, embeddings_metadata, embeddings_data, update_freq)\u001b[0m\n\u001b[1;32m   1017\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1018\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplugins\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprojector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1019\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.contrib'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-a2aec548b9ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m                 callbacks=[TensorBoardColabCallback(tboard)])\n\u001b[1;32m     22\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_compiled_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-a2aec548b9ea>\u001b[0m in \u001b[0;36mget_compiled_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                          \u001b[0mwrite_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                          \u001b[0mwrite_grads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                          write_images=True)\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, log_dir, histogram_freq, batch_size, write_graph, write_grads, write_images, embeddings_freq, embeddings_layer_names, embeddings_metadata, embeddings_data, update_freq)\u001b[0m\n\u001b[1;32m   1018\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplugins\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprojector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m             raise ImportError('You need the TensorFlow module installed to '\n\u001b[0m\u001b[1;32m   1021\u001b[0m                               'use TensorBoard.')\n\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: You need the TensorFlow module installed to use TensorBoard.",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def get_compiled_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(10, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    tbCallBack = TensorBoard(log_dir='./log',\n",
    "                             histogram_freq=1,\n",
    "                             write_graph=True,\n",
    "                             write_grads=True,\n",
    "                             write_images=True)\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'],\n",
    "                  verbose=1,\n",
    "                  callbacks=[TensorBoardColabCallback(tboard)])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_compiled_model()\n",
    "history = model.fit(train_dataset, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f66NhxBskjSy"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "loss = history.history['acc']\n",
    "val_loss = history.history['val_acc']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "plt.figure(figsize=(17,10))\n",
    "plt.plot(epochs, loss, 'r', label='Training accuracy')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "21Ru5CEBjBVi"
   },
   "source": [
    "# **TensorFlow Dataset API for GPU and TPU processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ksFTnFuOgxw0"
   },
   "source": [
    "<br> <br> <br> \n",
    "**Load Data frame and Make Tensor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f8sGa54nmXn0"
   },
   "outputs": [],
   "source": [
    "data_slices = tf.data.Dataset.from_tensor_slices(dict(df))\n",
    "for feature_batch in data_slices.take(1):\n",
    "  for key, value in feature_batch.items():\n",
    "    print(\"  {!r:20s}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0yBAZjIwgHiQ"
   },
   "source": [
    "**Load CSV data and create a tensor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GFAyS6LtyS_6"
   },
   "outputs": [],
   "source": [
    "def get_dataset(file_path, **kwargs):\n",
    "  dataset = tf.data.experimental.make_csv_dataset(\n",
    "      file_path,\n",
    "      batch_size=5, # Artificially small to make examples easier to show.\n",
    "      label_name=\"LABEL\",\n",
    "      na_value=\"?\",\n",
    "      num_epochs=1,\n",
    "      ignore_errors=True, \n",
    "      **kwargs)\n",
    "  return dataset\n",
    "\n",
    "raw_train_data = get_dataset(\"/content/BigData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SMC-U-vHdlLZ"
   },
   "outputs": [],
   "source": [
    "def show_batch(dataset):\n",
    "  for batch, label in dataset.take(1):\n",
    "    for key, value in batch.items():\n",
    "      print(\"{:20s}: {}\".format(key,value.numpy()))\n",
    "\n",
    "show_batch(raw_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hgxUtIKshrb8"
   },
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iROIM3VBZRiC"
   },
   "outputs": [],
   "source": [
    "processed_df, counter = applyAlgorithms(dataframe=df,algorithms=algos,column_list=cols)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM6Rx7mnWxXXwBcFriBlBWo",
   "collapsed_sections": [
    "60hBMzpOiCKW",
    "V31nPf4uiI5h",
    "9owgBBGNiRD1",
    "YiJ8kv8SinXC",
    "OrUbJOexWXI2",
    "21Ru5CEBjBVi"
   ],
   "include_colab_link": true,
   "name": "Tensorflow-MDM",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
